{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([layer.supports_masking for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "history = model.fit(train_seq_data, epochs=epochs, use_multiprocessing = True, \n",
    "                    workers=20, verbose = 1, validation_data = test_seq_data,\n",
    "                    callbacks=[checkpoint_callback, model_checkpoint])\n",
    "end = time.time()\n",
    "print(\"Time took {:3.1f} min\".format((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, GRU, Embedding, Bidirectional, TimeDistributed, BatchNormalization, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tqdm import tqdm\n",
    "# from keras_tqdm import TQDMNotebookCallback\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psutil\n",
    "# Ignore harmless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    for gpu in gpu_devices[6:]:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print('Using CPU')\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    print('used: {}% free: {:.2f}GB'.format(psutil.virtual_memory().percent, float(psutil.virtual_memory().free)/1024**3))#@ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use some GPUs\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[6:], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "    # Visible devices must be set at program startup\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "print('used: {}% free: {:.2f}GB'.format(psutil.virtual_memory().percent, float(psutil.virtual_memory().free)/1024**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get News headlines\n",
    "def get_news():\n",
    "    import h5py\n",
    "    import s3fs\n",
    "    data = pd.DataFrame()\n",
    "    s3 = s3fs.S3FileSystem(anon=False, key='AKIAQEOT5SDPIYYS2YV5', \n",
    "                           secret='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',\n",
    "                           client_kwargs={'region_name':'us-east-1'})\n",
    "    with h5py.File(s3.open(\"modelsdatabucket/news_db.h5\", 'rb'), 'r', lib_version='latest') as f:\n",
    "        tickers = list(f.keys())\n",
    "        for i in tqdm(tickers):\n",
    "            temp_df = pd.DataFrame(f[i+'/table'].value)# took one day to realize\n",
    "            temp_df = temp_df[['index', 'versionCreated', 'text', 'ticker']]\n",
    "            temp_df = temp_df.rename(columns={'index':'time', 'text': 'headline'})\n",
    "            temp_df[temp_df.columns[temp_df.dtypes == object]] = temp_df.select_dtypes([object]).stack().str.decode('utf-8').unstack()\n",
    "            temp_df.time = pd.to_datetime(temp_df.time, unit='ns')\n",
    "            temp_df.versionCreated=  pd.to_datetime(temp_df.versionCreated, unit='ns')\n",
    "            temp_df = temp_df.set_index('time')\n",
    "            print()\n",
    "            data = data.append(temp_df)\n",
    "        data = data.sort_index()\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "# Prepare News headlines\n",
    "def clean_text(df, column):\n",
    "    import re \n",
    "    #(\"\".join(headline)).strip()\n",
    "    headline = []\n",
    "    for i in df[column].apply(lambda x: '<s>'+x+'<\\s>'):\n",
    "        headline.append(i)\n",
    "    return headline\n",
    "\n",
    "# Encode to integers by using ascii 128\n",
    "def encode2bytes(text):\n",
    "    #text = tf.strings.unicode_split(text, 'UTF-8').to_list()\n",
    "    final_list = []\n",
    "    for sent in text:\n",
    "        temp_list = []\n",
    "        for char in sent:\n",
    "            if ord(char) < 128 :\n",
    "                temp_list.append(ord(char))\n",
    "        final_list.append(temp_list)\n",
    "    return final_list\n",
    "\n",
    "def split_X_y(text):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in text:\n",
    "        X.append(i[0:-1])\n",
    "        y.append(i[1:])\n",
    "    return X,y\n",
    "\n",
    "def create_language_model(batch_size):\n",
    "    model = Sequential(name = 'CharLSTM')\n",
    "    model.add(Embedding(127, 256,batch_input_shape=[batch_size, None], \n",
    "                        mask_zero=True, name ='EmbedLayer'))\n",
    "    model.add(Bidirectional(LSTM(1024, return_sequences=True,stateful=False,\n",
    "                                 recurrent_initializer='glorot_uniform'), merge_mode ='ave',name = 'BiLSTM'))\n",
    "    model.add(TimeDistributed(Dense(127, name = 'TimeDistDense')))\n",
    "    model.compile(optimizer=tf.optimizers.SGD(learning_rate=1e-2), \n",
    "                  loss = tf.losses.SparseCategoricalCrossentropy(from_logits = True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info(memory_usage = 1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data.headline.duplicated()]\n",
    "data.info(memory_usage = 1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clean_text(data, 'headline')\n",
    "b_text = encode2bytes(text)\n",
    "max_sentence_len = max(map(len, b_text))\n",
    "print(max_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[:5])\n",
    "print('<>'*50)\n",
    "print(b_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_X_y(b_text)\n",
    "num = np.random.randint(0, len(X))\n",
    "print('This is an example of the training sequence encoded as bytes:\\n')\n",
    "print(X[num])\n",
    "print(text[num])\n",
    "print(y[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(X, maxlen = max_sentence_len, padding = 'post')\n",
    "y = pad_sequences(y, maxlen = max_sentence_len, padding = 'post')\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(text) * 95//100\n",
    "train_seq_data = tf.data.Dataset.from_tensor_slices((X[:train_size],y[:train_size]))\n",
    "test_seq_data = tf.data.Dataset.from_tensor_slices((X[train_size:],y[train_size:]))\n",
    "train_seq_data, test_seq_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Check train set:')\n",
    "for input_txt, target_txt in  train_seq_data.take(1):\n",
    "    print('--------------------------------Headline--------------------------------')\n",
    "    print(input_txt.numpy())\n",
    "    print(\"\".join(map(chr, input_txt.numpy())))\n",
    "#     print(''.join(index2char[input_txt.numpy()]))\n",
    "    print('\\n')\n",
    "    print(target_txt.numpy())\n",
    "    print(\"\".join(map(chr, target_txt.numpy())))\n",
    "\n",
    "print('Check test set:')\n",
    "for input_txt, target_txt in  test_seq_data.take(1):\n",
    "    print('--------------------------------Headline--------------------------------')\n",
    "    print(input_txt.numpy())\n",
    "    print(\"\".join(map(chr, input_txt.numpy())))\n",
    "#     print(''.join(index2char[input_txt.numpy()]))\n",
    "    print('\\n')\n",
    "    print(target_txt.numpy())\n",
    "    print(\"\".join(map(chr, target_txt.numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mini-Batching/Subsequencing\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "train_seq_data = train_seq_data.batch(batch_size, drop_remainder=True)\n",
    "test_seq_data = test_seq_data.batch(batch_size, drop_remainder=True)\n",
    "print('Train Set Shape: ', train_seq_data, '\\nTest Set Shape: ', test_seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def configure_dataset(dataset):\n",
    "  return dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "train_seq_data = configure_dataset(train_seq_data)\n",
    "test_seq_data = configure_dataset(test_seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the checkpoint files and save each weights at each epoch\n",
    "checkpoint_dir = './training_checkpoints_CharWeights2'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    verbose=1,\n",
    "    save_weights_only=True)\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                               filepath='CharLM.h5',\n",
    "                               verbose=1,\n",
    "                               monitor='val_loss',\n",
    "                               mode='min',\n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "# with tf.device('GPU:1'):\n",
    "model = create_language_model(batch_size)\n",
    "weights_dir = './training_checkpoints_CharWeights'\n",
    "print(tf.train.latest_checkpoint(weights_dir))\n",
    "model.load_weights(tf.train.latest_checkpoint(weights_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([layer.supports_masking for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "history = model.fit(train_seq_data, epochs=epochs, use_multiprocessing = True, \n",
    "                    workers=20, verbose = 1, validation_data = test_seq_data,\n",
    "                    callbacks=[checkpoint_callback, model_checkpoint])\n",
    "end = time.time()\n",
    "print(\"Time took {:3.1f} min\".format((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "%notebook inline\n",
    "\n",
    "\n",
    "print('Test loss:', score)\n",
    "\n",
    "plt.figure(figsize=(20,10));\n",
    "labels = [\"loss\",\"val_loss\"]\n",
    "for lab in labels:\n",
    "    plt.plot(history.history[lab],label=lab + \" CharModel\");\n",
    "plt.xlabel('Number of Epoch')\n",
    "plt.ylabel('Loss Train/Validation')\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.savefig('Byte_embeds.png', dpi = 250, bbox_inches='tight');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_seq_data, verbose=2)\n",
    "print('Test loss:', score)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
