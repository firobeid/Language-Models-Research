{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Level Language Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras.backend as K\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psutil\n",
    "# Ignore harmless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n",
      "used: 83.2% free: 1.24GB\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    for gpu in gpu_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print('Using CPU')\n",
    "    tf.config.optimizer.set_jit(True) #activate Accelerated Linear Algebra Processor (XLA) through C API\n",
    "    print('used: {}% free: {:.2f}GB'.format(psutil.virtual_memory().percent, float(psutil.virtual_memory().free)/1024**3))#@ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "K.clear_session()\n",
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDEA FORMULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01001000 01100101 01101100 01101100 01101111'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_bitseq(s: str) -> str:\n",
    "    if not s.isascii():\n",
    "        raise ValueError(\"ASCII only allowed\")\n",
    "    return \" \".join(f\"{ord(i):08b}\" for i in s)\n",
    "make_bitseq('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Bits : 64\n",
      "7 Bits : 128\n",
      "8 Bits : 256\n"
     ]
    }
   ],
   "source": [
    "def n_possible_values(nbits: int) -> int:\n",
    "    return 2 ** nbits\n",
    "print('6 Bits :', n_possible_values(6))\n",
    "print('7 Bits :', n_possible_values(7))\n",
    "print('8 Bits :', n_possible_values(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary number: 1010\n",
      "Decimal number: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert(binary_number):\n",
    "    binary = binary_number\n",
    "    i = 0\n",
    "    decimal_number = 0\n",
    "    while (binary_number != 0):\n",
    "        c = int(binary_number % 10)\n",
    "        decimal_number = decimal_number + c * (2 ** i)\n",
    "        i +=1\n",
    "        binary_number = binary_number / 10\n",
    "    print('Binary number: %d' % binary)\n",
    "    print('Decimal number: %d' % decimal_number)\n",
    "    return 0\n",
    "convert(1010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 * 2**3 + 0 * 2**2 + 0 * 2**1 + 1 * 2**0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Bottom Line:\" Aim is to represent characters that require \"1 Byte\" and of only \"7 Bits slots\" (Not 8 Bits as we are used too) covering all english language alphabets, numerics and symbols up too decimal point \"127\". Decimal point 0 is reserved for padding and whenever model sees a 0 it will ignore it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "NEWS_STORE = Path('../news_db.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 542/542 [05:32<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1153304 entries, 2020-09-01 21:20:54 to 2021-01-20 00:04:31\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   text    1153304 non-null  object\n",
      " 1   ticker  1153304 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 26.4+ MB\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(NEWS_STORE) as store:\n",
    "    keys = list(store.keys())\n",
    "    news = pd.DataFrame()\n",
    "    for i in tqdm(keys): \n",
    "        news = news.append(store[i])  \n",
    "\n",
    "news = news.reset_index().set_index('versionCreated')\n",
    "news.index = news.index.strftime('%Y-%m-%d %H:%M:%S')\n",
    "news.index = pd.to_datetime(news.index)\n",
    "news = news.drop(['index','storyId', 'sourceCode'], axis = 1)\n",
    "news.index.name = 'time'\n",
    "news.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ''\n",
    "# Count Unique Characters\n",
    "for doc in text:\n",
    "    for s in doc:\n",
    "        txt += s\n",
    "chars = sorted(set(txt))\n",
    "print(chars)\n",
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARSE STOP-END TOKENS & ENCODE(1-BYTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clean_text(news, 'text')  #---> <s> headline <\\s> and clean\n",
    "b_text = encode2bytes(text) #----->ordinal encoding\n",
    "max_sentence_len = max([len(sentence) for sentence in b_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLIT INPUT / TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of the training sequence encoded as bytes:\n",
      "\n",
      "[60, 115, 62, 82, 80, 84, 45, 87, 69, 83, 84, 74, 69, 84, 32, 67, 69, 79, 32, 83, 65, 89, 83, 32, 84, 72, 69, 82, 69, 32, 73, 83, 32, 34, 86, 69, 82, 89, 32, 77, 65, 74, 79, 82, 32, 74, 79, 66, 34, 32, 84, 79, 32, 66, 69, 32, 68, 79, 78, 69, 32, 84, 79, 32, 71, 73, 86, 69, 32, 80, 69, 79, 80, 76, 69, 32, 67, 79, 78, 70, 73, 68, 69, 78, 67, 69, 32, 84, 72, 65, 84, 32, 67, 72, 65, 78, 71, 69, 83, 32, 84, 72, 65, 84, 32, 72, 65, 86, 69, 32, 66, 69, 69, 78, 32, 77, 65, 68, 69, 32, 87, 73, 76, 76, 32, 77, 65, 75, 69, 32, 55, 51, 55, 32, 77, 65, 88, 32, 84, 72, 69, 32, 83, 65, 70, 69, 83, 84, 32, 78, 65, 82, 82, 79, 87, 66, 79, 68, 89, 32, 68, 79, 77, 69, 83, 84, 73, 67, 32, 65, 73, 82, 67, 82, 65, 70, 84, 60, 92, 115]\n",
      "<s>RPT-WESTJET CEO SAYS THERE IS \"VERY MAJOR JOB\" TO BE DONE TO GIVE PEOPLE CONFIDENCE THAT CHANGES THAT HAVE BEEN MADE WILL MAKE 737 MAX THE SAFEST NARROWBODY DOMESTIC AIRCRAFT<\\s>\n",
      "[115, 62, 82, 80, 84, 45, 87, 69, 83, 84, 74, 69, 84, 32, 67, 69, 79, 32, 83, 65, 89, 83, 32, 84, 72, 69, 82, 69, 32, 73, 83, 32, 34, 86, 69, 82, 89, 32, 77, 65, 74, 79, 82, 32, 74, 79, 66, 34, 32, 84, 79, 32, 66, 69, 32, 68, 79, 78, 69, 32, 84, 79, 32, 71, 73, 86, 69, 32, 80, 69, 79, 80, 76, 69, 32, 67, 79, 78, 70, 73, 68, 69, 78, 67, 69, 32, 84, 72, 65, 84, 32, 67, 72, 65, 78, 71, 69, 83, 32, 84, 72, 65, 84, 32, 72, 65, 86, 69, 32, 66, 69, 69, 78, 32, 77, 65, 68, 69, 32, 87, 73, 76, 76, 32, 77, 65, 75, 69, 32, 55, 51, 55, 32, 77, 65, 88, 32, 84, 72, 69, 32, 83, 65, 70, 69, 83, 84, 32, 78, 65, 82, 82, 79, 87, 66, 79, 68, 89, 32, 68, 79, 77, 69, 83, 84, 73, 67, 32, 65, 73, 82, 67, 82, 65, 70, 84, 60, 92, 115, 62]\n"
     ]
    }
   ],
   "source": [
    "X, y = split_X_y(b_text)\n",
    "num = np.random.randint(0, len(X))\n",
    "print('This is an example of the training sequence encoded as bytes:\\n')\n",
    "print(X[num])\n",
    "print(text[num])\n",
    "print(y[num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PADDING \n",
    "Masking is a way to tell sequence-processing layers that certain timesteps in an input are missing, and thus should be skipped when processing the data.\n",
    "\n",
    "Padding is a special form of masking where the masked steps are at the start or at the beginning of a sequence. Padding comes from the need to encode sequence data into contiguous batches: in order to make all sequences in a batch fit a given standard length, it is necessary to pad or truncate some sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(976043, 519) (976043, 519)\n"
     ]
    }
   ],
   "source": [
    "X = pad_sequences(X, maxlen = max_sentence_len, padding = 'post')\n",
    "y = pad_sequences(y, maxlen = max_sentence_len, padding = 'post')\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST & VALIDATION SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_train = 959651\n",
    "train_size = length_train * 90//100\n",
    "\n",
    "validation_seq_data = tf.data.Dataset.from_tensor_slices((X[train_size:length_train + 1],y[train_size:length_train + 1]))\n",
    "test_seq_data = tf.data.Dataset.from_tensor_slices((X[length_train + 1: ],y[length_train + 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check VALIDATION set:\n",
      "--------------------------------Headline--------------------------------\n",
      "[ 60 115  62  84  82  65  67  84  79  82  32  83  85  80  80  76  89  32\n",
      "  45  32  83  69  69  83  32  50  48  49  57  32  69  65  82  78  73  78\n",
      "  71  83  32  80  69  82  32  68  73  76  85  84  69  68  32  83  72  65\n",
      "  82  69  32  36  52  46  54  53  32  45  32  36  52  46  55  53  60  92\n",
      " 115   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "<s>TRACTOR SUPPLY - SEES 2019 EARNINGS PER DILUTED SHARE $4.65 - $4.75<\\s\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\n",
      "\n",
      "[115  62  84  82  65  67  84  79  82  32  83  85  80  80  76  89  32  45\n",
      "  32  83  69  69  83  32  50  48  49  57  32  69  65  82  78  73  78  71\n",
      "  83  32  80  69  82  32  68  73  76  85  84  69  68  32  83  72  65  82\n",
      "  69  32  36  52  46  54  53  32  45  32  36  52  46  55  53  60  92 115\n",
      "  62   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "s>TRACTOR SUPPLY - SEES 2019 EARNINGS PER DILUTED SHARE $4.65 - $4.75<\\s>\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n"
     ]
    }
   ],
   "source": [
    "print('Check VALIDATION set:')\n",
    "for input_txt, target_txt in  validation_seq_data.take(1):\n",
    "    print('--------------------------------Headline--------------------------------')\n",
    "    print(input_txt.numpy())\n",
    "    print(\"\".join(map(chr, input_txt.numpy())))\n",
    "#     print(''.join(index2char[input_txt.numpy()]))\n",
    "    print('\\n')\n",
    "    print(target_txt.numpy())\n",
    "    print(\"\".join(map(chr, target_txt.numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Shape:  <BatchDataset shapes: ((256, 519), (256, 519)), types: (tf.int32, tf.int32)> \n",
      "Test Set Shape:  <BatchDataset shapes: ((256, 519), (256, 519)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "#Mini-Batching/Subsequencing\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "validation_seq_data = validation_seq_data.batch(batch_size, drop_remainder=True)\n",
    "test_seq_data = test_seq_data.batch(batch_size, drop_remainder=True)\n",
    "print('Validation Set Shape: ', validation_seq_data, '\\nTest Set Shape: ', test_seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Buffer for data prefetching \n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def configure_dataset(dataset):\n",
    "  return dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "validation_seq_data = configure_dataset(validation_seq_data)\n",
    "test_seq_data = configure_dataset(test_seq_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Models/Byte_embeds.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD LANGUAGE MODEL\n",
    "* If you want to compile: Compile then load in that order only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('CharLangModel.h5', compile=False)\n",
    "model.build(tf.TensorShape([256,None]))\n",
    "#compile below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_complile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x29973a16898>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_weights = './training_checkpoints_CharWeights'\n",
    "model.load_weights(tf.train.latest_checkpoint(trained_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374/374 [==============================] - 30055s 80s/step - loss: 0.0095 - sparse_categorical_accuracy: 0.1564\n",
      "Test loss: 0.009494862519204617\n",
      "Test accuracy: 0.15639889240264893\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(validation_seq_data, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_complile(model, sparse_acc = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1531ab34208>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_weights = './training_checkpoints_CharWeights'\n",
    "model.load_weights(tf.train.latest_checkpoint(trained_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 5429s 85s/step - loss: 0.0155 - accuracy: 0.1557\n",
      "Test loss: 0.015461504459381104\n",
      "Test accuracy: 0.1557215005159378\n"
     ]
    }
   ],
   "source": [
    "validation_score = model.evaluate(test_seq_data, verbose=1)\n",
    "print('Test loss:', validation_score[0])\n",
    "print('Test accuracy:', validation_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Observation: I wont judge this model based on its accuracy as not only is my data sparse, but also some features are not present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqye_characters = set(x for l in b_text for x in l)\n",
    "len(uniqye_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 216, 127)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline = \"<s>ZOETIS INC <ZTS.N>: CREDIT SUISSE RAISES PRICE TARGET TO $192 FROM $182 ZOETIS INC <ZTS.N>: BOFA GLOBAL RESEARCH RAISES PRICE OBJECTIVE TO $175 FROM $170 NYSE ORDER IMBALANCE <ZTS.N> 77562.0 SHARES ON SELL SIDE<\\s>\"\n",
    "# Encode UTF-8 ordinal level\n",
    "headline = encode2bytes(headline)\n",
    "#Split INput text and Output target\n",
    "X, y_true = headline[:-1], headline[1:]\n",
    "# Convert to array and squeeze dimension over axis = 0\n",
    "X = tf.expand_dims(X, 0)\n",
    "# Predict Ouput\n",
    "prediction = model.predict(X.numpy())\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logits (as is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:       <s>ZOETIS INC <ZTS.N>: CREDIT SUISSE RAISES PRICE TARGET TO $192 FROM $182 ZOETIS INC <ZTS.N>: BOFA GLOBAL RESEARCH RAISES PRICE OBJECTIVE TO $175 FROM $170 NYSE ORDER IMBALANCE <ZTS.N> 77562.0 SHARES ON SELL SIDE<\\s\n",
      "Prediction:  s>JOETIS INC <XTS.N>: CREDIT SUISSE RAISES PRICE TARGET TO $192 FROM $182 QOETIS INC <XTS.N>: BOFA GLOBAL RESEARCH RAISES PRICE OB,ECTIVE TO $175 FROM $170 NYSE ORDER IMBALANCE <ZTS.N> J7562.0 SHARES ON SELL SIDE<\\s>\n",
      "Actual:      s>ZOETIS INC <ZTS.N>: CREDIT SUISSE RAISES PRICE TARGET TO $192 FROM $182 ZOETIS INC <ZTS.N>: BOFA GLOBAL RESEARCH RAISES PRICE OBJECTIVE TO $175 FROM $170 NYSE ORDER IMBALANCE <ZTS.N> 77562.0 SHARES ON SELL SIDE<\\s>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:      \", \"\".join(map(chr, np.squeeze(X))))\n",
    "print(\"Prediction: \" ,\"\".join(map(chr,np.argmax(prediction, axis = -1).squeeze())))\n",
    "print(\"Actual:     \" ,\"\".join(map(chr,np.squeeze(y_true))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction[-1,:,:]\n",
    "p_i = np.zeros((prediction.shape))\n",
    "for i in range(0, len(headline[:-1])):\n",
    "    p = np.exp(prediction[i])/np.sum(np.exp(prediction[i])) #softmax\n",
    "    p_i[i] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115  62  74  79  69  84  73  83  32  73  78  67  32  60  88  84  83  46\n",
      "  78  62  58  32  67  82  69  68  73  84  32  83  85  73  83  83  69  32\n",
      "  82  65  73  83  69  83  32  80  82  73  67  69  32  84  65  82  71  69\n",
      "  84  32  84  79  32  36  49  57  50  32  70  82  79  77  32  36  49  56\n",
      "  50  32  81  79  69  84  73  83  32  73  78  67  32  60  88  84  83  46\n",
      "  78  62  58  32  66  79  70  65  32  71  76  79  66  65  76  32  82  69\n",
      "  83  69  65  82  67  72  32  82  65  73  83  69  83  32  80  82  73  67\n",
      "  69  32  79  66  44  69  67  84  73  86  69  32  84  79  32  36  49  55\n",
      "  53  32  70  82  79  77  32  36  49  55  48  32  78  89  83  69  32  79\n",
      "  82  68  69  82  32  73  77  66  65  76  65  78  67  69  32  60  90  84\n",
      "  83  46  78  62  32  74  55  53  54  50  46  48  32  83  72  65  82  69\n",
      "  83  32  79  78  32  83  69  76  76  32  83  73  68  69  60  92 115  62]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s>JOETIS INC <XTS.N>: CREDIT SUISSE RAISES PRICE TARGET TO $192 FROM $182 QOETIS INC <XTS.N>: BOFA GLOBAL RESEARCH RAISES PRICE OB,ECTIVE TO $175 FROM $170 NYSE ORDER IMBALANCE <ZTS.N> J7562.0 SHARES ON SELL SIDE<\\\\s>'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.argmax(p_i, axis = 1))\n",
    "''.join(map(chr,np.argmax(p_i, axis = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s>JOETIY INC <%TS.N>: CREDIT SUISSE RAISES TRICE TARGET TO $172 FROM $582 JOETIS INC <YTS.N>) BOFA GLOBAL RESEARCH RAISES PRICE OBWECTIIE TO $145 FROM $1%0 NYSE ORDER IMBAAANCE FBTS.N> 6U562.0 SHARES ON SELL SIDE<\\\\\\\\>'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(prediction, num_samples=1) \n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "\"\".join(map(chr,sampled_indices))\n",
    "# wont use sampling in my case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Today<>>>>>>>>>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use generate new text function to test on model\n",
    "generate_text(model, '<s> Today', 10, 1)\n",
    "#Concluded model is stateless and only learned how to represent and regenerate passed text but not generate new text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMBEDDINGS LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_embeddings = model.get_layer('EmbedLayer').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127, 256)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 60, 115,  62,  65,  77,  90,  78,  32, 119, 111, 110, 116,  32,\n",
       "        98, 101,  32, 112,  97, 121, 105, 110, 103,  32,  97, 110, 121,\n",
       "        32, 116,  97, 120, 101, 115,  32, 102, 111, 114,  32,  50,  48,\n",
       "        49,  57, 126,  60,  92, 115])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq = '<s>AMZN wont be paying any taxes for 2019~<\\s'\n",
    "input_seq = tf.squeeze(encode2bytes(input_seq)).numpy()\n",
    "input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(45, 256), dtype=float32, numpy=\n",
       "array([[-0.13882878, -0.30200124,  0.08077791, ...,  0.16514868,\n",
       "         0.18841438,  0.09872594],\n",
       "       [-0.11984932, -0.17366889,  0.0651598 , ...,  0.12940577,\n",
       "        -0.2054549 , -0.05256342],\n",
       "       [ 0.04611618,  0.08065684,  0.1510432 , ...,  0.01680804,\n",
       "        -0.09226788, -0.03016171],\n",
       "       ...,\n",
       "       [-0.13882878, -0.30200124,  0.08077791, ...,  0.16514868,\n",
       "         0.18841438,  0.09872594],\n",
       "       [-0.10369939, -0.25992334,  0.02147431, ...,  0.1398279 ,\n",
       "         0.11564761,  0.15265961],\n",
       "       [-0.11984932, -0.17366889,  0.0651598 , ...,  0.12940577,\n",
       "        -0.2054549 , -0.05256342]], dtype=float32)>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process of representing each of our features/character/byte/input\n",
    "lookup_table = tf.nn.embedding_lookup(trained_embeddings, input_seq)\n",
    "lookup_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(trained_embeddings[60]) == np.all(lookup_table[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell:\n",
    "\n",
    "For each character/byte the model looks up the embedding, runs the LSTM one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character/Byte. This distribution, for each predicted character/byte, is defined by the logits over the characters(i.e 1-126 Decimal Points bytes(0 is reserved for padding))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Embeddings Representations and Visualize in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127, 256)\n"
     ]
    }
   ],
   "source": [
    "import io, csv\n",
    "\n",
    "# save model weights\n",
    "\n",
    "print(trained_embeddings.shape) # shape: (characters/bytes, embedding_dim) -->(127,256)\n",
    "\n",
    "# save embeddings.\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "tsv_writer = csv.writer(out_m, delimiter='\\t')\n",
    "\n",
    "\n",
    "for i in range(0,127):\n",
    "    if i == 0: continue # skip 0, it's padding.\n",
    "    vec = trained_embeddings[i] \n",
    "    tsv_writer.writerow(str(chr(i)))\n",
    "#     out_m.write(chr(i+1), lineterminator='\\n')# skip 0, it's padding.255 last vector\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click me [Embeddings Projector](https://projector.tensorflow.org/) to visualize embeddings in 3D.\n",
    "\n",
    "For ready pretrained models [TensorFlow Hub](https://tfhub.dev/)\n",
    "\n",
    "I changed encoding scheme to cover UTF-8 encoded characters and the implemetation is found in the Scripts file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"RNNStocks\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "EmbedLayer (Embedding)       (None, None, 256)         32512     \n",
      "_________________________________________________________________\n",
      "BiLSTM (Bidirectional)       (None, 2048)              10493952  \n",
      "_________________________________________________________________\n",
      "BatchNormal (BatchNormalizat (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "FullConnected (Dense)        (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "BatchNormal2 (BatchNormaliza (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 11,586,305\n",
      "Trainable params: 11,581,185\n",
      "Non-trainable params: 5,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "daily = tf.keras.models.load_model('daily.h5', compile=False)\n",
    "daily.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 543/543 [05:08<00:00,  1.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import datetime\n",
    "with pd.HDFStore('../news_db.h5', mode = 'r') as store:\n",
    "    final_df = pd.DataFrame()\n",
    "    for i in tqdm(store.keys()):\n",
    "        df = store[i]\n",
    "        yesterday = (datetime.datetime.now() + datetime.timedelta(-1)).strftime('%Y-%m-%d')\n",
    "#        if df.loc[:, 'versionCreated'].apply(lambda x: x.strftime('%Y-%m-%d')) == yesterday:\n",
    "        news_yesterday = df.loc[df.loc[:, 'versionCreated'].apply(lambda x: x.strftime('%Y-%m-%d')) == yesterday]\n",
    "        news_yesterday = news_yesterday[~news_yesterday.text.duplicated()]\n",
    "        if len(news_yesterday) > 0:\n",
    "            sample = news_yesterday.text.str.cat(sep=' ')\n",
    "            if len(sample) > 1000: sample = sample[:500] + \"...\" + sample[-500:]\n",
    "            else: pass   \n",
    "            temp_df = pd.DataFrame.from_dict({i.strip('/'): sample}, orient='index').rename(columns={0:'Headlines'})\n",
    "            final_df = final_df.append(temp_df)\n",
    "            final_df['Date'] = yesterday\n",
    "            final_df = final_df[~final_df.Headlines.duplicated()]\n",
    "        else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n_Characters</th>\n",
       "      <td>292.0</td>\n",
       "      <td>361.304795</td>\n",
       "      <td>341.57085</td>\n",
       "      <td>20.0</td>\n",
       "      <td>93.75</td>\n",
       "      <td>215.5</td>\n",
       "      <td>550.75</td>\n",
       "      <td>1003.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count        mean        std   min    25%    50%     75%     max\n",
       "n_Characters  292.0  361.304795  341.57085  20.0  93.75  215.5  550.75  1003.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['n_Characters'] = final_df['Headlines'].str.len()\n",
    "final_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = encode2bytes(final_df.Headlines.apply(lambda x: '<s>' + x + '<\\s'))\n",
    "X = pad_sequences(X, maxlen =  max(map(len, X)), padding = 'post', truncating='post')\n",
    "\n",
    "predictions = daily.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Predictions'] = np.squeeze(predictions)\n",
    "final_df['Prediction Date'] = (datetime.datetime.now()).strftime('%Y-%m-%d')\n",
    "final_df['BUY(20% Threshold)'] = (np.squeeze(predictions) > 0.2)\n",
    "final_df['BUY(40% Threshold)'] = (np.squeeze(predictions) > 0.4)\n",
    "final_df['BUY(60% Threshold)'] = (np.squeeze(predictions) > 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[final_df['Headlines'].str.contains('NYSE ORDER IMBALANCE')][['Headlines', 'Predictions', 'Prediction Date']].to_csv('ORDER_IMBALANCES.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Words:\n",
    "*  blank check company\n",
    "* SPAC\n",
    "* 13F,G OR D\n",
    "* Clinical Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Date</th>\n",
       "      <th>n_Characters</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Prediction Date</th>\n",
       "      <th>BUY(20% Threshold)</th>\n",
       "      <th>BUY(40% Threshold)</th>\n",
       "      <th>BUY(60% Threshold)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Headlines, Date, n_Characters, Predictions, Prediction Date, BUY(20% Threshold), BUY(40% Threshold), BUY(60% Threshold)]\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_df[final_df['Headlines'].str.contains('13F')][['Headlines', 'Predictions', 'Prediction Date']].to_csv('key_words.csv')\n",
    "final_df[final_df['Headlines'].str.contains('biotech')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('Todays_Prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"Amazon's Sales Growth Costs a Fortune in Shipping and Fulfillment\" + \" Jeff Bezos, Bill Gates and other tech luminaries react to Biden's victory\" + \" Amazon rolls out rewards program that makes it easier for drivers to get work\" + \" TECH Alibaba cloud growth outpaces Amazon and Microsoft as Chinese tech giant pushes for profitability\"\n",
    "# sample = \"Joe Biden\" \n",
    "# sample = \"Donald Trump\" \n",
    "sample = '<s>' + sample + '<\\s' \n",
    "print(sample)\n",
    "sample = encode2bytes(sample)\n",
    "print(sample)\n",
    "sample = tf.ragged.constant(sample)\n",
    "sample = tf.squeeze(sample, )\n",
    "sample = tf.expand_dims(sample, 0).numpy()\n",
    "print(sample)\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability from Headlines: 0.099795\n"
     ]
    }
   ],
   "source": [
    "predict = daily(sample).numpy()[0][0]\n",
    "print(\"Probability from Headlines: %f\" % predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BRAZIL\\'S SUPREME COURT MAJORITY RULES TO ALLOW LOCAL GOVERNMENTS TO BYPASS FEDERAL GOVERNMENT TO BUY VACCINE IF IT FAILS TO DO SO NOVAVAX EXEC SAYS EXPECTS TO START PEDIATRIC STUDIES FOR ITS COVID-19 VACCINE IN SPRING - HOUSE SUBCOMMITTEE HEARING EXECS OF MODERNA, PFIZER, J&J, NOVAVAX AND ASTRAZENECA SAY THEY DO NOT CURRENTLY FORESEE ANY RAW MATERIAL SHORTAGES FOR MAKING THEIR COVID-19 VACCINES IN U.S. - HOUSE SUBCOMMITTEE HEARING EXCLUSIVE-AstraZeneca to miss second-quarter EU vaccine supply ta...O SOON RELAX SOME COVID GUIDELINES FOR PEOPLE WHO\\'VE BEEN VACCINATED -CNN INTERVIEW Israel giving \"symbolic\" amounts of COVID vaccines to Honduras, others CORRECTED-UPDATE 4-More German state workers to get AstraZeneca jab as doses go begging CANADA SET TO RECEIVE RECORD 640,000 VACCINE DOSES THIS WEEK UPDATE 2-Pfizer to ship 13 mln COVID-19 vaccine doses per week to U.S. by mid-March, says executive Common side effects of Moderna and Pfizer-BioNTech COVID vaccines, ways to deal with them: media'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[final_df.index == 'MRNA'].Headlines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribute Computations on Devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')]"
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU'):\n",
    "    xs = np.zeros((len(uniqye_characters),1))\n",
    "    h_prev = np.zeros((10,1))\n",
    "    Wxh = np.random.randn(10, len(uniqye_characters))*0.01 # input to hidden\n",
    "    Whh = np.random.randn(10, 10)*0.01 # hidden to hidden\n",
    "    Why = np.random.randn(len(uniqye_characters), 10)*0.01 # hidden to output\n",
    "    bh = np.zeros((10, 1)) # hidden bias\n",
    "    by = np.zeros((len(uniqye_characters), 1)) # output bias\n",
    "    hs = np.tanh(np.dot(Wxh, xs) + np.dot(Whh, h_prev) + bh) # hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env export > environment.yml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
