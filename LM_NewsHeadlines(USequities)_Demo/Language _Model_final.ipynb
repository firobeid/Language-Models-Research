{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Level Language Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Objective:\n",
    "___\n",
    "* P(Price Direction = 0 or 1 | NewsHeadlines) = Probability of News Headlines Affecting Stock Price to move Up or Down after certain time horizon.\n",
    "\n",
    "Please read `Language Model Report.pdf`for detailed report.\n",
    "\n",
    "\n",
    "## Files Directory:\n",
    "____\n",
    "__training_checkpoints_CharWeights__ Folder that has model weights(parameters) that the model takes on to power its predictive capabilities\n",
    "\n",
    "__Language Model.ipynb__: The notbook that governs this automation(as so called)\n",
    " \n",
    "__news_headlines.csv__: CSV file that has the news headlines per stock ticker per provided dates\n",
    "\n",
    "__functions.py__ All utility functions that assist in delivering the notebook outcome\n",
    "\n",
    "__CharLangModel.h5__ DeepLearning charactyer level language model that learns \"financial\" news headlines(text) dense representations by learning how to regenerate same text(news headline) passed to it\n",
    "\n",
    "__daily.h5__ DeepLearning Model that predicts asset price direction (0:Down, 1:Up) once given a news headline as inputm at T-1 to give back prediction at T\n",
    "\n",
    "__environment.env___ File that contains all enviroment dependencies for running the notebook (instruction below)\n",
    "## Enviroment Setup and Instruction:\n",
    "____\n",
    "1. To create a python virtual enviroment, preferablly in anaconda as its manages virtual envs efficiently, pass the following command in the anaconda terminal after cd into the folder that has the `environment.yml` file in, by passing the following command:\n",
    "\n",
    "`conda env create -f environment.yml`\n",
    "\n",
    "2. Make sure you are in the enviroment you just created or the one that has the packages that enables you to run the notebook.\n",
    "\n",
    "3. Unzip the attached automation folder `requirements.zip` and put all files in one folder and find the above file directory to familiarize yourself with the automation. Make sure the jupyter notebook (`Language _Model.ipynb`) is in same folder as all the files you just unzipped.\n",
    "\n",
    "## How to Use the Resource\n",
    "_________________________\n",
    "This notebook shows a proof of cencept of state-of-art techniques applied by top trading firms to leverage deep learning techniques in order to process natural language to a fine grained character (byte) level that can be more efficient then word vectors(embeddings) in terms in its compactness and closer to a generalized contextual representation for financial news data.\n",
    "\n",
    "Feel free to check my github for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### __Disclaimer__:\n",
    "- The models are not to be used for any sort of investment advice as the notebook is a proof of concept of hows to leverage Natural Lamguage Processing using state of the art Deep Learning (LSTM architecture).\n",
    "- The accuracy of the model is only justfied based on the proof of concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras.backend as K\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psutil\n",
    "# Ignore harmless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "2.3.0-tf\n",
      "1.19.5\n"
     ]
    }
   ],
   "source": [
    "# Make sure you have these package versions\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pull in my helper hardcoded functions from `functions.py` script, make sure this script in same directory as this jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in my helper hardcoded functions from functions.py script \n",
    "from functions import clean_text,encode2bytes,split_X_y, model_complile, generate_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check where your notebook's location/path/directory is, type this in code cell: `pwd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd #RUN THIS CODE CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')]\n",
      "1 Physical GPU, 1 Logical GPUs\n",
      "used: 72.5% free: 4.34GB\n"
     ]
    }
   ],
   "source": [
    "#Set Processor to Run computations in backend\n",
    "print(tf.config.list_physical_devices(device_type=None))\n",
    "tf.config.optimizer.set_jit(True)\n",
    "gpus = tf.config.list_physical_devices('XLA_CPU') #Our normal laptops have Accelerated Linear Algebra Processor (XLA) activate it through C API\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use some XLA_CPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[:], 'XLA_CPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('XLA_CPU')\n",
    "        print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        print('used: {}% free: {:.2f}GB'.format(psutil.virtual_memory().percent, float(psutil.virtual_memory().free)/1024**3))#@ \n",
    "    except RuntimeError as e:\n",
    "    # Visible devices must be set at program startup\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make sure you put the right path for the new_headlines, CharLangModel & daily model files :`new_headlines.csv` - `CharLangModel.h5` - `daily.h5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for same reproducibility as my results due to stochastic nature of start point\n",
    "K.clear_session()\n",
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "#Get Data: MAKE SURE THE FILES YOUR ARE LOADING IN ARE IN SAME FOLDER AS YOU JUPYTER SO CODE BELOW WORKS\n",
    "# ../:to run from terminal -- ./: to run from jupyter\n",
    "NEWS_STORE = Path(\".\",\"new_headlines.csv\") \n",
    "CharLangModel = Path('.','CharLangModel.h5')\n",
    "daily_model = Path('.','daily.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDEA FORMULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01001000 01100101 01101100 01101100 01101111'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_bitseq(s: str) -> str:\n",
    "    if not s.isascii():\n",
    "        raise ValueError(\"ASCII only allowed\")\n",
    "    return \" \".join(f\"{ord(i):08b}\" for i in s)\n",
    "make_bitseq('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Bits : 64\n",
      "7 Bits : 128\n",
      "8 Bits : 256\n"
     ]
    }
   ],
   "source": [
    "def n_possible_values(nbits: int) -> int:\n",
    "    return 2 ** nbits\n",
    "print('6 Bits :', n_possible_values(6))\n",
    "print('7 Bits :', n_possible_values(7))\n",
    "print('8 Bits :', n_possible_values(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary number: 1010\n",
      "Decimal number: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert(binary_number):\n",
    "    binary = binary_number\n",
    "    i = 0\n",
    "    decimal_number = 0\n",
    "    while (binary_number != 0):\n",
    "        c = int(binary_number % 10)\n",
    "        decimal_number = decimal_number + c * (2 ** i)\n",
    "        i +=1\n",
    "        binary_number = binary_number / 10\n",
    "    print('Binary number: %d' % binary)\n",
    "    print('Decimal number: %d' % decimal_number)\n",
    "    return 0\n",
    "convert(1010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 * 2**3 + 0 * 2**2 + 0 * 2**1 + 1 * 2**0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Bottom Line:\" Aim is to represent characters that require \"1 Byte\" and of only \"7 Bits slots\" (Not 8 Bits as we are used too) covering all english language alphabets, numerics and symbols up too decimal point \"127\". Decimal point 0 is reserved for padding and whenever model sees a 0 it will ignore it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1536339 entries, 2020-09-01 21:20:54 to 2021-07-20 03:53:56\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   text    1536339 non-null  object\n",
      " 1   ticker  1536339 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 35.2+ MB\n"
     ]
    }
   ],
   "source": [
    "news= pd.read_csv(NEWS_STORE)\n",
    "news = news.set_index('time')\n",
    "news.index = pd.to_datetime(news.index)\n",
    "news.index = news.index.strftime('%Y-%m-%d %H:%M:%S')\n",
    "# news = news.drop(['index', 'time'], axis = 1)\n",
    "# news.index.name = 'time'\n",
    "news.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DOWNSIZE FOR MEMORY EFFICIENCY:\n",
    "If you want to downsize the `news` --> Pass in this code: `news = news.iloc[0:int(0.25*len(news))]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 384084 entries, 2020-09-01 21:20:54 to 2020-10-22 21:00:00\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    384084 non-null  object\n",
      " 1   ticker  384084 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 8.8+ MB\n"
     ]
    }
   ],
   "source": [
    "news.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARSE STOP-END TOKENS & ENCODE(1-BYTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\xa0', '¡', '£', '¥', '§', '\\xad', '®', '²', '´', '¹', '½', '¿', 'Á', 'Ã', 'Ä', 'Å', 'Ç', 'È', 'É', 'Ê', 'Í', 'Ï', 'Ñ', 'Ó', 'Ô', 'Õ', 'Ö', 'Ø', 'Ú', 'Ü', 'à', 'á', 'â', 'ã', 'ä', 'å', 'ç', 'è', 'é', 'ê', 'ë', 'í', 'ï', 'ñ', 'ó', 'ô', 'õ', 'ö', 'ø', 'ú', 'ü', 'ý', 'ć', 'Ē', 'İ', 'Ł', 'ł', 'ń', 'ō', 'Ś', 'Ş', 'ş', 'š', 'ū', 'ż', 'Ž', 'ʰ', '̧', 'Β', 'Μ', 'ა', 'ბ', 'გ', 'დ', 'ე', 'ვ', 'ზ', 'თ', 'ი', 'კ', 'ლ', 'მ', 'ნ', 'ო', 'რ', 'ს', 'ტ', 'უ', 'შ', 'ც', 'ძ', 'ხ', 'ᵗ', '\\u200a', '\\u200b', '\\u200d', '‐', '‑', '‒', '–', '—', '‘', '’', '“', '”', '„', '‡', '•', '…', '\\u202f', '′', '″', '\\u2066', '€', '₹', '℠', '™', '↑', '→', '∙', '≥', 'Ⓡ', '\\u3000', '、', 'い', 'け', 'た', 'ち', 'で', 'に', 'の', 'ま', 'み', 'む', 'め', 'り', 'る', 'を', 'ウ', 'ス', 'ダ', 'ベ', 'ー', '上', '与', '业', '个', '中', '为', '云', '交', '产', '人', '伸', '保', '値', '决', '到', '協', '博', '去', '参', '反', '发', '取', '各', '合', '品', '商', '報', '塑', '売', '备', '宣', '小', '巡', '布', '幅', '年', '建', '待', '後', '性', '情', '意', '戦', '护', '整', '新', '方', '日', '易', '更', '最', '期', '末', '株', '案', '極', '氨', '注', '消', '润', '漆', '热', '牌', '的', '目', '社', '立', '米', '終', '組', '続', '经', '络', '网', '者', '联', '聚', '膜', '获', '营', '落', '行', '見', '解', '議', '设', '貿', '费', '路', '退', '选', '通', '連', '過', '酯', '重', '錯', '面', '高', '\\ue60c', 'ﬀ', '️', '\\ufeff', '＆', '（', '）', '１', '２', '４', '８', '９', 'Ａ', 'Ｎ', 'Ｐ', 'Ｓ', 'Ｙ', '�']\n",
      "355\n"
     ]
    }
   ],
   "source": [
    "txt = ''\n",
    "# Count Unique Characters\n",
    "for doc in news.text:\n",
    "    for s in doc:\n",
    "        txt += s\n",
    "chars = sorted(set(txt))\n",
    "print(chars)\n",
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clean_text(news, 'text')  #---> <s> headline <\\s> and clean\n",
    "b_text = encode2bytes(text) #----->ordinal encoding\n",
    "max_sentence_len = max(map(len,b_text))\n",
    "#max([len(sentence) for sentence in b_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqye_characters = set(x for l in b_text for x in l)\n",
    "len(uniqye_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLIT INPUT / TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of the training sequence encoded as bytes:\n",
      "\n",
      "[60, 115, 62, 65, 109, 97, 122, 111, 110, 32, 65, 110, 110, 111, 117, 110, 99, 101, 115, 32, 70, 105, 114, 115, 116, 32, 82, 111, 98, 111, 116, 105, 99, 115, 32, 70, 117, 108, 102, 105, 108, 108, 109, 101, 110, 116, 32, 67, 101, 110, 116, 101, 114, 32, 105, 110, 32, 76, 111, 117, 105, 115, 105, 97, 110, 97, 60, 92, 115]\n",
      "<s>Amazon Announces First Robotics Fulfillment Center in Louisiana<\\s>\n",
      "[115, 62, 65, 109, 97, 122, 111, 110, 32, 65, 110, 110, 111, 117, 110, 99, 101, 115, 32, 70, 105, 114, 115, 116, 32, 82, 111, 98, 111, 116, 105, 99, 115, 32, 70, 117, 108, 102, 105, 108, 108, 109, 101, 110, 116, 32, 67, 101, 110, 116, 101, 114, 32, 105, 110, 32, 76, 111, 117, 105, 115, 105, 97, 110, 97, 60, 92, 115, 62]\n"
     ]
    }
   ],
   "source": [
    "X, y = split_X_y(b_text)\n",
    "num = np.random.randint(0, len(X))\n",
    "print('This is an example of the training sequence encoded as bytes:\\n')\n",
    "print(X[num])\n",
    "print(text[num])\n",
    "print(y[num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PADDING \n",
    "Masking is a way to tell sequence-processing layers that certain timesteps in an input are missing, and thus should be skipped when processing the data.\n",
    "\n",
    "Padding is a special form of masking where the masked steps are at the start or at the beginning of a sequence. Padding comes from the need to encode sequence data into contiguous batches: in order to make all sequences in a batch fit a given standard length, it is necessary to pad or truncate some sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384084, 516) (384084, 516)\n"
     ]
    }
   ],
   "source": [
    "X = pad_sequences(X, maxlen = max_sentence_len, padding = 'post')\n",
    "y = pad_sequences(y, maxlen = max_sentence_len, padding = 'post')\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST & VALIDATION SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_train = X.shape[0]\n",
    "train_size = length_train * 90//100\n",
    "\n",
    "validation_seq_data = tf.data.Dataset.from_tensor_slices((X[train_size:length_train + 1],y[train_size:length_train + 1]))\n",
    "test_seq_data = tf.data.Dataset.from_tensor_slices((X[length_train + 1: ],y[length_train + 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check VALIDATION set:\n",
      "--------------------------------Headline--------------------------------\n",
      "[ 60 115  62  66  82  73  69  70  45  67  97 114 110 105 118  97 108  32\n",
      "  67 111 114 112 111 114  97 116 105 111 110  32  38  32  80  76  67  32\n",
      "  85 112 100  97 116 101  32  79 110  32  67 121  98 101 114  32  69 118\n",
      " 101 110 116  60  92 115   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "<s>BRIEF-Carnival Corporation & PLC Update On Cyber Event<\\s\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "\n",
      "\n",
      "[115  62  66  82  73  69  70  45  67  97 114 110 105 118  97 108  32  67\n",
      " 111 114 112 111 114  97 116 105 111 110  32  38  32  80  76  67  32  85\n",
      " 112 100  97 116 101  32  79 110  32  67 121  98 101 114  32  69 118 101\n",
      " 110 116  60  92 115  62   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "s>BRIEF-Carnival Corporation & PLC Update On Cyber Event<\\s>\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n"
     ]
    }
   ],
   "source": [
    "print('Check VALIDATION set:')\n",
    "for input_txt, target_txt in  validation_seq_data.take(1):\n",
    "    print('--------------------------------Headline--------------------------------')\n",
    "    print(input_txt.numpy())\n",
    "    print(\"\".join(map(chr, input_txt.numpy())))\n",
    "#     print(''.join(index2char[input_txt.numpy()]))\n",
    "    print('\\n')\n",
    "    print(target_txt.numpy())\n",
    "    print(\"\".join(map(chr, target_txt.numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Shape:  <BatchDataset shapes: ((256, 516), (256, 516)), types: (tf.int32, tf.int32)> \n",
      "Test Set Shape:  <BatchDataset shapes: ((256, 516), (256, 516)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "#Mini-Batching/Subsequencing\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "validation_seq_data = validation_seq_data.batch(batch_size, drop_remainder=True)\n",
    "test_seq_data = test_seq_data.batch(batch_size, drop_remainder=True)\n",
    "print('Validation Set Shape: ', validation_seq_data, '\\nTest Set Shape: ', test_seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Buffer for data prefetching \n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def configure_dataset(dataset):\n",
    "    return dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "validation_seq_data = configure_dataset(validation_seq_data)\n",
    "test_seq_data = configure_dataset(test_seq_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD LANGUAGE MODEL\n",
    "* If you want to compile: Compile then load in that order only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(CharLangModel, compile=False)\n",
    "#If you get error in his step make sure your numpy version in '1.21.1'(numpy.__version__)\n",
    "model.build(tf.TensorShape([256,None]))\n",
    "#compile below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_complile(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make sure you put the right path for the trained weights folder :`training_checkpoints_CharWeights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x24104180f40>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_weights = './training_checkpoints_CharWeights'\n",
    "model.load_weights(tf.train.latest_checkpoint(trained_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints_CharWeights\\\\ckpt_20'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(trained_weights) #take a look to what we are fetching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set Evaluation\n",
    "\n",
    "###### This is a memory consuming step, feek free to skip it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 3442s 23s/step - loss: 0.0101 - sparse_categorical_accuracy: 0.1530\n",
      "Test loss: 0.01008651778101921\n",
      "Test accuracy: 0.15295174717903137\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(validation_seq_data, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note the input is sparese and some categories are missing since we only filtered on english language used characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model again to use on inference data\n",
    "model_complile(model, sparse_acc = False)\n",
    "trained_weights = './training_checkpoints_CharWeights'\n",
    "model.load_weights(tf.train.latest_checkpoint(trained_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 216, 127)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to use the models on future data inputs\n",
    "headline = \"<s>ZOETIS INC <ZTS.N>: CREDIT SUISSE RAISES PRICE TARGET TO $192 FROM $182 ZOETIS INC <ZTS.N>: BOFA GLOBAL RESEARCH RAISES PRICE OBJECTIVE TO $175 FROM $170 NYSE ORDER IMBALANCE <ZTS.N> 77562.0 SHARES ON SELL SIDE<\\s>\"\n",
    "# Encode UTF-8 ordinal level\n",
    "headline = encode2bytes(headline)\n",
    "#Split INput text and Output target\n",
    "X, y_true = headline[:-1], headline[1:]\n",
    "# Convert to array and squeeze dimension over axis = 0\n",
    "X = tf.expand_dims(X, 0)\n",
    "# Predict Ouput\n",
    "prediction = model.predict(X.numpy())\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logits (as is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:       <s>ZOETIS INC <ZTS.N>: CREDIT SUISSE RAISES PRICE TARGET TO $192 FROM $182 ZOETIS INC <ZTS.N>: BOFA GLOBAL RESEARCH RAISES PRICE OBJECTIVE TO $175 FROM $170 NYSE ORDER IMBALANCE <ZTS.N> 77562.0 SHARES ON SELL SIDE<\\s\n",
      "Prediction:  s>JOETIS INC <XTS.N>: CREDIT SUISSE RAISES PRICE TARGET TO $192 FROM $182 QOETIS INC <XTS.N>: BOFA GLOBAL RESEARCH RAISES PRICE OB,ECTIVE TO $175 FROM $170 NYSE ORDER IMBALANCE <ZTS.N> J7562.0 SHARES ON SELL SIDE<\\s>\n",
      "Actual:      s>ZOETIS INC <ZTS.N>: CREDIT SUISSE RAISES PRICE TARGET TO $192 FROM $182 ZOETIS INC <ZTS.N>: BOFA GLOBAL RESEARCH RAISES PRICE OBJECTIVE TO $175 FROM $170 NYSE ORDER IMBALANCE <ZTS.N> 77562.0 SHARES ON SELL SIDE<\\s>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:      \", \"\".join(map(chr, np.squeeze(X))))\n",
    "print(\"Prediction: \" ,\"\".join(map(chr,np.argmax(prediction, axis = -1).squeeze())))\n",
    "print(\"Actual:     \" ,\"\".join(map(chr,np.squeeze(y_true))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction[-1,:,:]\n",
    "p_i = np.zeros((prediction.shape))\n",
    "for i in range(0, len(headline[:-1])):\n",
    "    p = np.exp(prediction[i])/np.sum(np.exp(prediction[i])) #softmax\n",
    "    p_i[i] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115  62  74  79  69  84  73  83  32  73  78  67  32  60  88  84  83  46\n",
      "  78  62  58  32  67  82  69  68  73  84  32  83  85  73  83  83  69  32\n",
      "  82  65  73  83  69  83  32  80  82  73  67  69  32  84  65  82  71  69\n",
      "  84  32  84  79  32  36  49  57  50  32  70  82  79  77  32  36  49  56\n",
      "  50  32  81  79  69  84  73  83  32  73  78  67  32  60  88  84  83  46\n",
      "  78  62  58  32  66  79  70  65  32  71  76  79  66  65  76  32  82  69\n",
      "  83  69  65  82  67  72  32  82  65  73  83  69  83  32  80  82  73  67\n",
      "  69  32  79  66  44  69  67  84  73  86  69  32  84  79  32  36  49  55\n",
      "  53  32  70  82  79  77  32  36  49  55  48  32  78  89  83  69  32  79\n",
      "  82  68  69  82  32  73  77  66  65  76  65  78  67  69  32  60  90  84\n",
      "  83  46  78  62  32  74  55  53  54  50  46  48  32  83  72  65  82  69\n",
      "  83  32  79  78  32  83  69  76  76  32  83  73  68  69  60  92 115  62]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s>JOETIS INC <XTS.N>: CREDIT SUISSE RAISES PRICE TARGET TO $192 FROM $182 QOETIS INC <XTS.N>: BOFA GLOBAL RESEARCH RAISES PRICE OB,ECTIVE TO $175 FROM $170 NYSE ORDER IMBALANCE <ZTS.N> J7562.0 SHARES ON SELL SIDE<\\\\s>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.argmax(p_i, axis = 1))\n",
    "''.join(map(chr,np.argmax(p_i, axis = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s>XOETIS INC <ZTS.N>: CVEDIT SUISSE RAISES PRICE TARGET TO $192 FROM $132 6OETIS INC <ZTS.N>: BOFA GLOBAL RESEARCH RAISES PRICE OB,ECTIXE TO $17Y 3ROM $120 NYSE ORDER IMUALANCE UUTS.N> 37592.0 SHARES ON SELL SIDE<\\\\s>'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(prediction, num_samples=1) \n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "\"\".join(map(chr,sampled_indices))\n",
    "# wont use sampling in my case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use generate new text function to test on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Today<>>>>>>>>>'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, '<s> Today', 10, 1)\n",
    "#Concluded model is stateless and only learned how to represent and regenerate passed text but not generate new text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Concluded__: model is stateless and only learned how to represent and regenerate passed text but not generate new text based on the past term/word!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMBEDDINGS LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_embeddings = model.get_layer('EmbedLayer').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127, 256)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 60, 115,  62,  65,  77,  90,  78,  32, 119, 111, 110, 116,  32,\n",
       "        98, 101,  32, 112,  97, 121, 105, 110, 103,  32,  97, 110, 121,\n",
       "        32, 116,  97, 120, 101, 115,  32, 102, 111, 114,  32,  50,  48,\n",
       "        49,  57, 126,  60,  92, 115])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq = '<s>AMZN wont be paying any taxes for 2019~<\\s'\n",
    "input_seq = tf.squeeze(encode2bytes(input_seq)).numpy()\n",
    "input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(45, 256), dtype=float32, numpy=\n",
       "array([[-0.13882878, -0.30200124,  0.08077791, ...,  0.16514868,\n",
       "         0.18841438,  0.09872594],\n",
       "       [-0.11984932, -0.17366889,  0.0651598 , ...,  0.12940577,\n",
       "        -0.2054549 , -0.05256342],\n",
       "       [ 0.04611618,  0.08065684,  0.1510432 , ...,  0.01680804,\n",
       "        -0.09226788, -0.03016171],\n",
       "       ...,\n",
       "       [-0.13882878, -0.30200124,  0.08077791, ...,  0.16514868,\n",
       "         0.18841438,  0.09872594],\n",
       "       [-0.10369939, -0.25992334,  0.02147431, ...,  0.1398279 ,\n",
       "         0.11564761,  0.15265961],\n",
       "       [-0.11984932, -0.17366889,  0.0651598 , ...,  0.12940577,\n",
       "        -0.2054549 , -0.05256342]], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process of representing each of our features/character/byte/input\n",
    "lookup_table = tf.nn.embedding_lookup(trained_embeddings, input_seq)\n",
    "lookup_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make sure language model embeddings is equal when we transfered them to the asset price prediction\n",
    "np.all(trained_embeddings[60]) == np.all(lookup_table[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In a nutshell:__\n",
    "\n",
    "For each character/byte the model looks up the embedding, runs the LSTM one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character/Byte. This distribution, for each predicted character/byte, is defined by the logits over the characters(i.e 1-126 Decimal Points bytes(0 is reserved for padding))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Embeddings Representations and Visualize in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, csv\n",
    "\n",
    "# save model weights\n",
    "\n",
    "print(trained_embeddings.shape) # shape: (characters/bytes, embedding_dim) -->(127,256)\n",
    "\n",
    "# save embeddings.\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "tsv_writer = csv.writer(out_m, delimiter='\\t')\n",
    "\n",
    "\n",
    "for i in range(0,127):\n",
    "    if i == 0: continue # skip 0, it's padding.\n",
    "    vec = trained_embeddings[i] \n",
    "    tsv_writer.writerow(str(chr(i)))\n",
    "#     out_m.write(chr(i+1), lineterminator='\\n')# skip 0, it's padding.255 last vector\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click me [Embeddings Projector](https://projector.tensorflow.org/) to visualize embeddings in 3D.\n",
    "Upload the `vecs.tsv` & `meta.tsv`, that where created as a result of running the above code, to the right place on the website to visualize the character level dense vectors in a compressed 3-D representations.\n",
    "\n",
    "For ready pretrained models [TensorFlow Hub](https://tfhub.dev/)\n",
    "\n",
    "I changed encoding scheme to cover UTF-8 encoded characters and the implemetation of how I encoded the characters to decimal point, byte level encoding, is found in the `functions.py` script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "Now we transfer the learning from the language model , that was able to draw connections between characters in financial news headlines context, to stablize the learning of the second, ultimate goal, model that predicts price directions of an asset based on news headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"RNNStocks\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "EmbedLayer (Embedding)       (None, None, 256)         32512     \n",
      "_________________________________________________________________\n",
      "BiLSTM (Bidirectional)       (None, 2048)              10493952  \n",
      "_________________________________________________________________\n",
      "BatchNormal (BatchNormalizat (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "FullConnected (Dense)        (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "BatchNormal2 (BatchNormaliza (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 11,586,305\n",
      "Trainable params: 11,581,185\n",
      "Non-trainable params: 5,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "daily = tf.keras.models.load_model(daily_model, compile=False) #make sure you have the right path for daily_model\n",
    "daily.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Make sure `new_headlines.csv` is in same path of notebook location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df= pd.read_csv(\"new_headlines.csv\")\n",
    "final_df = final_df.set_index('time')\n",
    "final_df.index = pd.to_datetime(final_df.index)\n",
    "final_df.index = final_df.index.strftime('%Y-%m-%d %H:%M:%S')\n",
    "final_df.index = pd.to_datetime(final_df.index)\n",
    "max_date = final_df.index.max().strftime('%Y-%m-%d')\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "final_df = final_df.reset_index().set_index(['time', 'ticker']).sort_index(level = 0, sort_remaining = 0).loc[idx[max_date,:], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n_Characters</th>\n",
       "      <td>2982.0</td>\n",
       "      <td>70.261234</td>\n",
       "      <td>33.093554</td>\n",
       "      <td>11.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>265.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count       mean        std   min   25%   50%   75%    max\n",
       "n_Characters  2982.0  70.261234  33.093554  11.0  47.0  67.0  84.0  265.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['n_Characters'] = final_df['text'].str.len()\n",
    "final_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = encode2bytes(final_df.text.apply(lambda x: '<s>' + x + '<\\s'))\n",
    "X = pad_sequences(X, maxlen =  max(map(len, X)), padding = 'post', truncating='post')\n",
    "\n",
    "predictions = daily.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Predictions'] = np.squeeze(predictions)\n",
    "final_df['Prediction Date'] = (datetime.datetime.now()).strftime('%Y-%m-%d')\n",
    "final_df['BUY(20% Threshold)'] = (np.squeeze(predictions) > 0.2)\n",
    "final_df['BUY(40% Threshold)'] = (np.squeeze(predictions) > 0.4)\n",
    "final_df['BUY(60% Threshold)'] = (np.squeeze(predictions) > 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>n_Characters</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Prediction Date</th>\n",
       "      <th>BUY(20% Threshold)</th>\n",
       "      <th>BUY(40% Threshold)</th>\n",
       "      <th>BUY(60% Threshold)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th>ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-07-20 00:02:21</th>\n",
       "      <th>JNJ</th>\n",
       "      <td>US: US states to unveil $35b opioid settlement</td>\n",
       "      <td>46</td>\n",
       "      <td>0.237652</td>\n",
       "      <td>2021-08-08</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-20 00:04:22</th>\n",
       "      <th>RSG</th>\n",
       "      <td>REPUBLIC SERVICES, INC. SEC Filings files Form...</td>\n",
       "      <td>51</td>\n",
       "      <td>0.132664</td>\n",
       "      <td>2021-08-08</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-20 00:05:07</th>\n",
       "      <th>CDNS</th>\n",
       "      <td>CADENCE DESIGN SYSTEMS INC SEC Filings files F...</td>\n",
       "      <td>54</td>\n",
       "      <td>0.054491</td>\n",
       "      <td>2021-08-08</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-20 00:05:43</th>\n",
       "      <th>PPG</th>\n",
       "      <td>(EN) PPG INDUSTRIES, INC. Monthly Presentation...</td>\n",
       "      <td>57</td>\n",
       "      <td>0.249839</td>\n",
       "      <td>2021-08-08</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-20 00:06:53</th>\n",
       "      <th>RSG</th>\n",
       "      <td>REPUBLIC SERVICES, INC. SEC Filings files Form...</td>\n",
       "      <td>51</td>\n",
       "      <td>0.132664</td>\n",
       "      <td>2021-08-08</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2021-07-20 21:47:13</th>\n",
       "      <th>GL</th>\n",
       "      <td>PDF 1: TORCHMARK CORPORATION (Gl-globe Life In...</td>\n",
       "      <td>108</td>\n",
       "      <td>0.112298</td>\n",
       "      <td>2021-08-08</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GL</th>\n",
       "      <td>TORCHMARK CORPORATION (Gl-globe Life Inc- Anno...</td>\n",
       "      <td>101</td>\n",
       "      <td>0.084044</td>\n",
       "      <td>2021-08-08</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-20 21:48:33</th>\n",
       "      <th>MRVL</th>\n",
       "      <td>Marvell Technology, Inc. SEC Filings files For...</td>\n",
       "      <td>52</td>\n",
       "      <td>0.072060</td>\n",
       "      <td>2021-08-08</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-20 21:54:52</th>\n",
       "      <th>IR</th>\n",
       "      <td>SPX Flow Denies to Sell Itself to Ingersoll Rand</td>\n",
       "      <td>48</td>\n",
       "      <td>0.114485</td>\n",
       "      <td>2021-08-08</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-20 21:56:25</th>\n",
       "      <th>ROST</th>\n",
       "      <td>Ross Stores (ROST) Expands Footprint, Unveils ...</td>\n",
       "      <td>55</td>\n",
       "      <td>0.120866</td>\n",
       "      <td>2021-08-08</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2982 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         text  n_Characters  Predictions Prediction Date  BUY(20% Threshold)  BUY(40% Threshold)  BUY(60% Threshold)\n",
       "time                ticker                                                                                                                                                          \n",
       "2021-07-20 00:02:21 JNJ        US: US states to unveil $35b opioid settlement            46     0.237652      2021-08-08                True               False               False\n",
       "2021-07-20 00:04:22 RSG     REPUBLIC SERVICES, INC. SEC Filings files Form...            51     0.132664      2021-08-08               False               False               False\n",
       "2021-07-20 00:05:07 CDNS    CADENCE DESIGN SYSTEMS INC SEC Filings files F...            54     0.054491      2021-08-08               False               False               False\n",
       "2021-07-20 00:05:43 PPG     (EN) PPG INDUSTRIES, INC. Monthly Presentation...            57     0.249839      2021-08-08                True               False               False\n",
       "2021-07-20 00:06:53 RSG     REPUBLIC SERVICES, INC. SEC Filings files Form...            51     0.132664      2021-08-08               False               False               False\n",
       "...                                                                       ...           ...          ...             ...                 ...                 ...                 ...\n",
       "2021-07-20 21:47:13 GL      PDF 1: TORCHMARK CORPORATION (Gl-globe Life In...           108     0.112298      2021-08-08               False               False               False\n",
       "                    GL      TORCHMARK CORPORATION (Gl-globe Life Inc- Anno...           101     0.084044      2021-08-08               False               False               False\n",
       "2021-07-20 21:48:33 MRVL    Marvell Technology, Inc. SEC Filings files For...            52     0.072060      2021-08-08               False               False               False\n",
       "2021-07-20 21:54:52 IR       SPX Flow Denies to Sell Itself to Ingersoll Rand            48     0.114485      2021-08-08               False               False               False\n",
       "2021-07-20 21:56:25 ROST    Ross Stores (ROST) Expands Footprint, Unveils ...            55     0.120866      2021-08-08               False               False               False\n",
       "\n",
       "[2982 rows x 7 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Make put the right path to where you like to save these output file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[final_df['text'].str.contains('NYSE ORDER IMBALANCE')][['text', 'Predictions', 'Prediction Date']].to_csv('ORDER_IMBALANCES.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Words:\n",
    "*  blank check company\n",
    "* SPAC\n",
    "* 13F,G OR D\n",
    "* Clinical Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Make sure you put the right path to where you like to save these output files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save your predictions to the path of you choice\n",
    "final_df.to_csv('Todays_Prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass in News Headline or Random Words to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Amazon's Sales Growth Costs a Fortune in Shipping and Fulfillment Jeff Bezos, Bill Gates and other tech luminaries react to Biden's victory Amazon rolls out rewards program that makes it easier for drivers to get work TECH Alibaba cloud growth outpaces Amazon and Microsoft as Chinese tech giant pushes for profitability<\\s\n",
      "[[60], [115], [62], [65], [109], [97], [122], [111], [110], [39], [115], [32], [83], [97], [108], [101], [115], [32], [71], [114], [111], [119], [116], [104], [32], [67], [111], [115], [116], [115], [32], [97], [32], [70], [111], [114], [116], [117], [110], [101], [32], [105], [110], [32], [83], [104], [105], [112], [112], [105], [110], [103], [32], [97], [110], [100], [32], [70], [117], [108], [102], [105], [108], [108], [109], [101], [110], [116], [32], [74], [101], [102], [102], [32], [66], [101], [122], [111], [115], [44], [32], [66], [105], [108], [108], [32], [71], [97], [116], [101], [115], [32], [97], [110], [100], [32], [111], [116], [104], [101], [114], [32], [116], [101], [99], [104], [32], [108], [117], [109], [105], [110], [97], [114], [105], [101], [115], [32], [114], [101], [97], [99], [116], [32], [116], [111], [32], [66], [105], [100], [101], [110], [39], [115], [32], [118], [105], [99], [116], [111], [114], [121], [32], [65], [109], [97], [122], [111], [110], [32], [114], [111], [108], [108], [115], [32], [111], [117], [116], [32], [114], [101], [119], [97], [114], [100], [115], [32], [112], [114], [111], [103], [114], [97], [109], [32], [116], [104], [97], [116], [32], [109], [97], [107], [101], [115], [32], [105], [116], [32], [101], [97], [115], [105], [101], [114], [32], [102], [111], [114], [32], [100], [114], [105], [118], [101], [114], [115], [32], [116], [111], [32], [103], [101], [116], [32], [119], [111], [114], [107], [32], [84], [69], [67], [72], [32], [65], [108], [105], [98], [97], [98], [97], [32], [99], [108], [111], [117], [100], [32], [103], [114], [111], [119], [116], [104], [32], [111], [117], [116], [112], [97], [99], [101], [115], [32], [65], [109], [97], [122], [111], [110], [32], [97], [110], [100], [32], [77], [105], [99], [114], [111], [115], [111], [102], [116], [32], [97], [115], [32], [67], [104], [105], [110], [101], [115], [101], [32], [116], [101], [99], [104], [32], [103], [105], [97], [110], [116], [32], [112], [117], [115], [104], [101], [115], [32], [102], [111], [114], [32], [112], [114], [111], [102], [105], [116], [97], [98], [105], [108], [105], [116], [121], [60], [92], [115]]\n",
      "[[ 60 115  62  65 109  97 122 111 110  39 115  32  83  97 108 101 115  32\n",
      "   71 114 111 119 116 104  32  67 111 115 116 115  32  97  32  70 111 114\n",
      "  116 117 110 101  32 105 110  32  83 104 105 112 112 105 110 103  32  97\n",
      "  110 100  32  70 117 108 102 105 108 108 109 101 110 116  32  74 101 102\n",
      "  102  32  66 101 122 111 115  44  32  66 105 108 108  32  71  97 116 101\n",
      "  115  32  97 110 100  32 111 116 104 101 114  32 116 101  99 104  32 108\n",
      "  117 109 105 110  97 114 105 101 115  32 114 101  97  99 116  32 116 111\n",
      "   32  66 105 100 101 110  39 115  32 118 105  99 116 111 114 121  32  65\n",
      "  109  97 122 111 110  32 114 111 108 108 115  32 111 117 116  32 114 101\n",
      "  119  97 114 100 115  32 112 114 111 103 114  97 109  32 116 104  97 116\n",
      "   32 109  97 107 101 115  32 105 116  32 101  97 115 105 101 114  32 102\n",
      "  111 114  32 100 114 105 118 101 114 115  32 116 111  32 103 101 116  32\n",
      "  119 111 114 107  32  84  69  67  72  32  65 108 105  98  97  98  97  32\n",
      "   99 108 111 117 100  32 103 114 111 119 116 104  32 111 117 116 112  97\n",
      "   99 101 115  32  65 109  97 122 111 110  32  97 110 100  32  77 105  99\n",
      "  114 111 115 111 102 116  32  97 115  32  67 104 105 110 101 115 101  32\n",
      "  116 101  99 104  32 103 105  97 110 116  32 112 117 115 104 101 115  32\n",
      "  102 111 114  32 112 114 111 102 105 116  97  98 105 108 105 116 121  60\n",
      "   92 115]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 326)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"Amazon's Sales Growth Costs a Fortune in Shipping and Fulfillment\" + \" Jeff Bezos, Bill Gates and other tech luminaries react to Biden's victory\" + \" Amazon rolls out rewards program that makes it easier for drivers to get work\" + \" TECH Alibaba cloud growth outpaces Amazon and Microsoft as Chinese tech giant pushes for profitability\"\n",
    "# sample = \"Joe Biden\" \n",
    "# sample = \"Donald Trump\" \n",
    "sample = '<s>' + sample + '<\\s' \n",
    "print(sample)\n",
    "sample = encode2bytes(sample)\n",
    "print(sample)\n",
    "# sample = tf.ragged.constant(sample)\n",
    "sample = tf.squeeze(sample, )\n",
    "sample = tf.expand_dims(sample, 0).numpy()\n",
    "print(sample)\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability from Headlines: 0.233987\n"
     ]
    }
   ],
   "source": [
    "predict = daily(sample).numpy()[0][0]\n",
    "print(\"Probability from Headlines: %f\" % predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribute Computations on Devices:\n",
    "Some tricks how you can fixate certain training on a specific processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU'):\n",
    "    xs = np.zeros((len(uniqye_characters),1))\n",
    "    h_prev = np.zeros((10,1))\n",
    "    Wxh = np.random.randn(10, len(uniqye_characters))*0.01 # input to hidden\n",
    "    Whh = np.random.randn(10, 10)*0.01 # hidden to hidden\n",
    "    Why = np.random.randn(len(uniqye_characters), 10)*0.01 # hidden to output\n",
    "    bh = np.zeros((10, 1)) # hidden bias\n",
    "    by = np.zeros((len(uniqye_characters), 1)) # output bias\n",
    "    hs = np.tanh(np.dot(Wxh, xs) + np.dot(Whh, h_prev) + bh) # hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saved packages to run this notebook\n",
    "#!conda env export > environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If interested please refer to my github: [My Github!](https://github.com/firobeid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pushstashenv",
   "language": "python",
   "name": "pushstashenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
